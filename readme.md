# Statistical Language Models for Turkish

**StatLM-Turk** provides robust implementations of syllable-based and character-based N-gram language models for Turkish text, complete with smoothing, perplexity evaluation, and sentence generation utilities.

---

## ğŸš€ Features

* **Data Preparation**: Clean, normalize, and syllabify Turkish text.
* **N-Gram Modeling**: Build unigram, bigram, and trigram models at both character and syllable levels.
* **Smoothing**: Apply Goodâ€“Turing smoothing to handle unseen Nâ€‘grams.
* **Perplexity Evaluation**: Compute and compare model perplexities on held-out data.
* **Sentence Generation**: Sample fluent Turkish sentences from any Nâ€‘gram model.

---

## ğŸ“¦ Prerequisites

* Python 3.8+
* pip (Python package manager)

---

## ğŸ”§ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/emretechno/Statistical-Language-Model.git
   cd Statistical-Language-Model
   ```
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ Usage

### 1. Data Preparation

Prepare your raw Turkish corpus (UTFâ€‘8 encoded). By default, the script expects data in `DataPreparation/`. Normalize and syllabify:

```bash
python DataPreparation/prepare_data.py \
  --input path/to/raw_corpus.txt \
  --output DataPreparation/processed_corpus.txt
```

### 2. Train Nâ€‘Gram Models

Generate character and syllable Nâ€‘gram counts with smoothing:

```bash
python NGramCalculation/train_models.py \
  --input DataPreparation/processed_corpus.txt \
  --output models/ \
  --max_n 3 \
  --smoothing good_turing
```

### 3. Evaluate Perplexity

Compute perplexity on a test set:

```bash
python NGramCalculation/evaluate_perplexity.py \
  --models models/ \
  --test_data path/to/test_corpus.txt
```

### 4. Generate Random Sentences

Sample sentences from any trained model:

```bash
python RandomSentence/generate.py \
  --model models/char_trigram.pkl \
  --num_sentences 5
```

---

## ğŸ“Š Results

* Detailed perplexity comparisons across models are saved to `results/perplexity_results.csv`.
* Randomly generated sentences for each Nâ€‘gram level appear in `results/generated_sentences.txt`.
* See `REPORT/analysis.pdf` for in-depth performance analysis and visualizations.

---

## ğŸ“ Project Structure

```
Statistical-Language-Model/
â”œâ”€â”€ DataPreparation/        # Text cleaning & syllable segmentation scripts
â”œâ”€â”€ NGramCalculation/       # Nâ€‘gram count, smoothing & evaluation modules
â”œâ”€â”€ RandomSentence/         # Sentence sampling utilities
â”œâ”€â”€ REPORT/                 # Analysis reports & figures
â”œâ”€â”€ models/                 # Trained model artifacts (output)
â”œâ”€â”€ results/                # Perplexity & generated sentences outputs
â”œâ”€â”€ run.py                  # Quick-run entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ LICENSE                 # Project license
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please fork, create a feature branch, and submit a pull request.
Ensure any new code includes appropriate tests and documentation.

---

